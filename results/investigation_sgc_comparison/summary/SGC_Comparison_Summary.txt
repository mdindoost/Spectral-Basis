======================================================================
SGC COMPARISON: COMPREHENSIVE SUMMARY
======================================================================

Research Question:
Can restricted eigenvectors + row normalization enhance SGC?

Two Methods Compared:
1. SGC Baseline: X → SGC diffusion → Logistic Regression (with bias)
2. Our Variant: X → SGC diffusion → Restricted Eigenvectors → Row Norm → Logistic Regression (no bias)

======================================================================
KEY FINDINGS
======================================================================

Total Experiments: 40
Datasets Tested: 8
K Values Tested per Dataset: 5

Improvements:
  Positive (RowNorm better): 18/40 (45.0%)
  Negative (SGC better): 22/40 (55.0%)

Average Improvement: -8.79pp
Median Improvement: -6.82pp
Best Improvement: +24.84pp
Worst Improvement: -49.86pp

======================================================================
PER-DATASET SUMMARY
======================================================================

AMAZON-COMPUTERS
----------------------------------------------------------------------
K values tested: [2, 4, 6, 8, 10]

Best SGC: 87.13% at K=2
Best RowNorm: 87.25% at K=4
Best Improvement: +12.14pp at K=10
Average Improvement: +6.21pp
Verdict: RowNorm variant shows improvement

CITESEER
----------------------------------------------------------------------
K values tested: [2, 4, 6, 8, 10]

Best SGC: 75.59% at K=4
Best RowNorm: 26.61% at K=8
Best Improvement: -48.72pp at K=8
Average Improvement: -49.18pp
Verdict: SGC baseline is superior

COAUTHOR-CS
----------------------------------------------------------------------
K values tested: [2, 4, 6, 8, 10]

Best SGC: 93.65% at K=2
Best RowNorm: 71.25% at K=2
Best Improvement: -20.16pp at K=10
Average Improvement: -21.38pp
Verdict: SGC baseline is superior

COAUTHOR-PHYSICS
----------------------------------------------------------------------
K values tested: [2, 4, 6, 8, 10]

Best SGC: 96.00% at K=2
Best RowNorm: 82.83% at K=2
Best Improvement: -11.70pp at K=10
Average Improvement: -12.46pp
Verdict: SGC baseline is superior

CORA
----------------------------------------------------------------------
K values tested: [2, 4, 6, 8, 10]

Best SGC: 88.44% at K=2
Best RowNorm: 62.94% at K=10
Best Improvement: -23.87pp at K=10
Average Improvement: -25.16pp
Verdict: SGC baseline is superior

OGBN-ARXIV
----------------------------------------------------------------------
K values tested: [2, 4, 6, 8, 10]

Best SGC: 59.95% at K=2
Best RowNorm: 68.40% at K=10
Best Improvement: +18.67pp at K=10
Average Improvement: +12.03pp
Verdict: RowNorm variant shows improvement

PUBMED
----------------------------------------------------------------------
K values tested: [2, 4, 6, 8, 10]

Best SGC: 81.09% at K=2
Best RowNorm: 85.82% at K=2
Best Improvement: +9.13pp at K=10
Average Improvement: +6.93pp
Verdict: RowNorm variant shows improvement

WIKICS
----------------------------------------------------------------------
K values tested: [2, 4, 6, 8, 10]

Best SGC: 76.60% at K=2
Best RowNorm: 79.70% at K=10
Best Improvement: +24.84pp at K=10
Average Improvement: +12.68pp
Verdict: RowNorm variant shows improvement

======================================================================
TECHNICAL NOTES
======================================================================

Hyperparameters:
  Learning Rate: 0.2 (SGC setting)
  Weight Decay: 5e-4
  Epochs: 100
  Optimizer: Adam

Data Splits:
  Random 60/20/20 splits
  5 splits × 5 seeds = 25 runs per configuration

SGC Diffusion:
  Operator: A_hat = D^(-1/2) (A + I) D^(-1/2)
  Propagation: X_tilde = A_hat^K @ X

Restricted Eigenvectors:
  Computed from diffused features X_tilde
  Solves: L_r @ v = λ D_r @ v
  D-orthonormality verified

======================================================================
CONCLUSIONS
======================================================================

The RowNorm variant DOES NOT enhance SGC. On average, it significantly
underperforms the SGC baseline across datasets and K values.

Possible reasons:
1. Row normalization removes magnitude information
2. No bias term constrains decision boundaries
3. Single-layer classifier lacks nonlinear capacity
4. Restricted eigenvectors from diffused features may not provide
   optimal basis for linear classification

Recommendation: SGC's simple approach is superior for this task.
