================================================================================
INVESTIGATION 5: MULTI-SCALE FEATURE FUSION
Comprehensive Analysis Report
================================================================================

RESEARCH QUESTION
--------------------------------------------------------------------------------
Does combining low-diffusion features (k=2) with high-diffusion restricted
eigenvectors (k=10) improve performance? Does augmenting with log-magnitude
(from Investigation 3) provide additional benefit?

METHODS TESTED
--------------------------------------------------------------------------------
1. SGC Baseline: Logistic regression on SGC k=2
2. Branch 1 Only: SGC k=2 + MLP
3. Branch 2 Only: Restricted k=10 + RowNorm + MLP
4. Dual-Branch: Fusion of Branch 1 + Branch 2
5. Triple-Branch: Branch 1 + Branch 2 (augmented with log-magnitude)

SUMMARY STATISTICS
--------------------------------------------------------------------------------
Total experiments: 9
Datasets: 9
Split types: ['fixed']

DUAL-BRANCH RESULTS
--------------------------------------------------------------------------------
Success rate (>0.5pp gain): 2 / 9 = 22.2%
Mean gain: -3.22pp (std: 7.46pp)
Median gain: -0.07pp
Range: [-21.54pp, 1.45pp]

Top 3 Dual-Branch gains:
  citeseer (fixed): +1.45pp
  amazon-computers (fixed): +0.58pp
  ogbn-arxiv (fixed): +0.40pp

TRIPLE-BRANCH RESULTS
--------------------------------------------------------------------------------
Success rate (>0.5pp gain): 3 / 9 = 33.3%
Mean gain: -3.21pp (std: 7.80pp)
Median gain: -0.08pp
Range: [-22.46pp, 1.96pp]

Top 3 Triple-Branch gains:
  citeseer (fixed): +1.96pp
  amazon-computers (fixed): +0.87pp
  ogbn-arxiv (fixed): +0.54pp

MAGNITUDE CONTRIBUTION (Triple - Dual)
--------------------------------------------------------------------------------
Positive effect (>0.5pp): 1 / 9 = 11.1%
Neutral effect (-0.5 to +0.5pp): 7 / 9
Negative effect (<-0.5pp): 1 / 9 = 11.1%
Mean contribution: +0.01pp (std: 0.42pp)
Median contribution: +0.14pp

Top 3 Magnitude contributions:
  citeseer (fixed): +0.51pp
  amazon-computers (fixed): +0.30pp
  cora (fixed): +0.28pp

Bottom 3 Magnitude contributions (where it hurt):
  pubmed (fixed): -0.92pp
  amazon-photo (fixed): -0.28pp
  coauthor-cs (fixed): -0.12pp

DETAILED RESULTS TABLE
--------------------------------------------------------------------------------
         Dataset Split  SGC Baseline  Branch 1 Only  Branch 2 Only  Dual-Branch  Triple-Branch  Dual Gain  Triple Gain  Magnitude Contribution
amazon-computers fixed     89.644705      90.821611      90.066615    91.398960      91.695038   0.577350     0.873427                0.296078
    amazon-photo fixed     93.841019      94.535742      89.993324    93.092855      92.812296  -1.442887    -1.723446               -0.280559
        citeseer fixed     70.015085      70.226246      66.244346    71.674210      72.187030   1.447964     1.960784                0.512820
     coauthor-cs fixed     94.372952      94.111228      80.567063    93.991271      93.871316  -0.119957    -0.239912               -0.119956
coauthor-physics fixed     95.994200      96.510141      92.208692    96.443473      96.426083  -0.066668    -0.084058               -0.017390
            cora fixed     78.316944      78.032789      63.584702    78.185796      78.469948   0.153006     0.437158                0.284152
      ogbn-arxiv fixed     58.133861      65.613235      69.962349    70.362737      70.505114   0.400388     0.542765                0.142376
          pubmed fixed     77.180004      75.960002      49.080002    54.420003      53.500003 -21.539999   -22.459999               -0.920000
          wikics fixed     76.369754      78.250401      63.148576    69.860104      70.023023  -8.390297    -8.227378                0.162919

KEY FINDINGS
--------------------------------------------------------------------------------
1. Multi-scale fusion (Dual-Branch) helps in 22% of cases
   → Multi-scale fusion has limited benefit

2. Log-magnitude augmentation (Triple-Branch) helps in 33% of cases
   → Magnitude augmentation rarely helps (11% positive)

3. Best performing method overall: Triple-Branch (4/9 datasets)

CONCLUSION
--------------------------------------------------------------------------------
Investigation 5 tested multi-scale feature fusion combining low-diffusion
(k=2) and high-diffusion (k=10) features with optional log-magnitude
augmentation. Results show:

- Multi-scale fusion (Dual-Branch) has limited benefit
- Log-magnitude augmentation rarely improves results

Best overall method: Triple-Branch

================================================================================
